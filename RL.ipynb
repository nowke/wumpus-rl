{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wumpus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = WumpusWorldScenario(\n",
    "    agent = Explorer(heading='north', verbose=False),\n",
    "    objects = [(Wumpus(),(1,3)),\n",
    "               (Pit(),(3,3)),\n",
    "               (Pit(),(3,1)),\n",
    "               (Gold(),(2,3))],\n",
    "    width = 4, \n",
    "    height = 4, \n",
    "    entrance = (1,1),\n",
    "    trace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WumpusWorld(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._reset()\n",
    "        self.actions = [\n",
    "            'TurnRight', 'TurnLeft', 'Forward', 'Grab', \n",
    "            'Climb', 'Shoot', 'Wait'\n",
    "        ]\n",
    "        self.action_space = spaces.Box(low=0, high=len(self.actions)-1, shape=(1,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.MultiDiscrete([5, 5, 2, 2, 2, 2, 2]) # TODO: Heading variable\n",
    "        \n",
    "    def step(self, action):\n",
    "        action = int(action)\n",
    "        if action >= len(self.actions) or action < 0:\n",
    "            action = 6 # Wait\n",
    "        self.env.execute_action(self.agent, self.actions[action])\n",
    "        self.env.time_step += 1\n",
    "        self.env.exogenous_change()\n",
    "        reward = self.agent.performance_measure - self.previous_score\n",
    "        \n",
    "        observation = self._state\n",
    "        \n",
    "        #### SPECIAL CASE reward #####\n",
    "        if self._location == (2,3) and not self.gold_reward_given: # TODO: Refactor hardcoded location\n",
    "            if action == 3:\n",
    "                self.has_gold = True\n",
    "                reward = 100 # Grab\n",
    "            else:\n",
    "                reward = 50   # Gold state\n",
    "            self.gold_reward_given = True\n",
    "        elif self._location == (1,1): # TODO: Refactor hardocded location\n",
    "            if self.has_gold:\n",
    "                reward = 50\n",
    "            elif action == 4: # Climb\n",
    "                reward = -200 # Don't climb without gold :-)\n",
    "        #### SPECIAL CASE reward #####\n",
    "            \n",
    "        self.previous_score = self.agent.performance_measure\n",
    "        done = self.env.is_done()\n",
    "        \n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.previous_score = 0\n",
    "        self._reset()\n",
    "        return self._state\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        print(self.env.to_string())\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.scenario = WumpusWorldScenario(\n",
    "            agent = Explorer(heading='north', verbose=False),\n",
    "            objects = [(Wumpus(),(1,3)),\n",
    "                       (Pit(),(3,3)),\n",
    "                       (Pit(),(3,1)),\n",
    "                       (Gold(),(2,3))],\n",
    "            width = 4, \n",
    "            height = 4, \n",
    "            entrance = (1,1),\n",
    "            trace=False\n",
    "        )\n",
    "        self.agent = self.scenario.agent\n",
    "        self.env = self.scenario.env\n",
    "        self.has_gold = False\n",
    "        self.gold_reward_given = False\n",
    "        \n",
    "    @property\n",
    "    def _state(self):\n",
    "        location = self._location\n",
    "        percept = self.env.percept(self.agent)\n",
    "        \n",
    "        return np.array([location[0], location[1], int(percept[0]), int(percept[1]),\n",
    "                int(percept[2]), int(percept[3]), int(percept[4])])\n",
    "        \n",
    "    @property\n",
    "    def _location(self): return self.agent.location\n",
    "    \n",
    "    @property\n",
    "    def _percept(self): return self.env.percept(self.agent)\n",
    "    \n",
    "    @property\n",
    "    def spec(self):\n",
    "        return WumpusWorld.Spec\n",
    "    \n",
    "    class Spec():\n",
    "        id = \"WumpusWorld\"\n",
    "        \n",
    "env = WumpusWorld()\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        #env.render()\n",
    "        #print(observation)\n",
    "        action = env.action_space.sample() #a\n",
    "        observation, reward, done, info = env.step(action) #s, r, s'\n",
    "        print(reward)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Input  input_layer: [None, 7]\n",
      "[TL] Dense  mlp_layer1: 64 relu\n",
      "[TL] Dense  mlp_layer2: 64 relu\n",
      "[TL] Dense  mlp_layer3: 64 relu\n",
      "[TL] Dense  mlp_layer4: 64 relu\n",
      "[TL] Dense  dense_7: 1 No Activation\n",
      "[TL] Input  input_layer: (None, 7)\n",
      "[TL] Dense  hidden_layer1: 64 relu\n",
      "[TL] Dense  hidden_layer2: 64 relu\n",
      "[TL] Dense  hidden_layer3: 64 relu\n",
      "[TL] Dense  hidden_layer4: 64 relu\n",
      "[TL] Dense  dense_8: 1 tanh\n",
      "Training...  | Algorithm: AC  | Environment: WumpusWorld\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 0/5000  | Episode Reward: -1003.0000  | Running Time: 0.2327\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_actor\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_critic\n",
      "[TL] [*] Saved\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 1/5000  | Episode Reward: -1003.0000  | Running Time: 0.5015\n",
      "Episode: 2/5000  | Episode Reward: -206.0000  | Running Time: 0.6710\n",
      "Episode: 3/5000  | Episode Reward: -209.0000  | Running Time: 0.8758\n",
      "Episode: 4/5000  | Episode Reward: -200.0000  | Running Time: 0.8970\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 5/5000  | Episode Reward: -1012.0000  | Running Time: 1.2289\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 6/5000  | Episode Reward: -1007.0000  | Running Time: 1.3731\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 7/5000  | Episode Reward: -1027.0000  | Running Time: 1.9991\n",
      "Episode: 8/5000  | Episode Reward: -200.0000  | Running Time: 2.0191\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 9/5000  | Episode Reward: -1031.0000  | Running Time: 2.6335\n",
      "Episode: 10/5000  | Episode Reward: -225.0000  | Running Time: 3.2830\n",
      "Episode: 11/5000  | Episode Reward: -202.0000  | Running Time: 3.3531\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 12/5000  | Episode Reward: -388.0000  | Running Time: 4.4308\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 13/5000  | Episode Reward: -1019.0000  | Running Time: 4.8096\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 14/5000  | Episode Reward: -1014.0000  | Running Time: 5.0928\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 15/5000  | Episode Reward: -1057.0000  | Running Time: 6.2052\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 16/5000  | Episode Reward: -1016.0000  | Running Time: 6.5526\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 17/5000  | Episode Reward: -1101.0000  | Running Time: 8.8702\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 18/5000  | Episode Reward: -1013.0000  | Running Time: 9.1388\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 19/5000  | Episode Reward: -1175.0000  | Running Time: 13.0519\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 20/5000  | Episode Reward: -912.0000  | Running Time: 16.3321\n",
      "Episode: 21/5000  | Episode Reward: -377.0000  | Running Time: 19.8260\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 22/5000  | Episode Reward: -1013.0000  | Running Time: 20.0953\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 23/5000  | Episode Reward: -1002.0000  | Running Time: 20.1359\n",
      "Episode: 24/5000  | Episode Reward: -200.0000  | Running Time: 24.3713\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 25/5000  | Episode Reward: -1102.0000  | Running Time: 26.3685\n",
      "Episode: 26/5000  | Episode Reward: -200.0000  | Running Time: 30.2994\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 27/5000  | Episode Reward: -1155.0000  | Running Time: 33.3479\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 28/5000  | Episode Reward: -1046.0000  | Running Time: 34.2721\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 29/5000  | Episode Reward: -1056.0000  | Running Time: 35.5484\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 30/5000  | Episode Reward: -1059.0000  | Running Time: 36.7679\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 31/5000  | Episode Reward: -1145.0000  | Running Time: 39.7080\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 32/5000  | Episode Reward: -1020.0000  | Running Time: 40.0982\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_actor\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_critic\n",
      "[TL] [*] Saved\n",
      "Episode: 33/5000  | Episode Reward: 463.0000  | Running Time: 44.3732\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_actor\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_critic\n",
      "[TL] [*] Saved\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 34/5000  | Episode Reward: -1103.0000  | Running Time: 47.1642\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_actor\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_critic\n",
      "[TL] [*] Saved\n",
      "Episode: 35/5000  | Episode Reward: -200.0000  | Running Time: 52.0303\n",
      "Episode: 36/5000  | Episode Reward: -200.0000  | Running Time: 56.2619\n",
      "Episode: 37/5000  | Episode Reward: -200.0000  | Running Time: 60.9052\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 38/5000  | Episode Reward: -1092.0000  | Running Time: 63.1986\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_actor\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/AC-WumpusWorld/model_critic\n",
      "[TL] [*] Saved\n",
      "Episode: 39/5000  | Episode Reward: -200.0000  | Running Time: 67.9718\n",
      "Episode: 40/5000  | Episode Reward: -200.0000  | Running Time: 72.2852\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 41/5000  | Episode Reward: -1033.0000  | Running Time: 73.1579\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "import tensorflow as tf\n",
    "from rlzoo.common.utils import make_env, set_seed\n",
    "from rlzoo.algorithms import AC\n",
    "from rlzoo.common.value_networks import ValueNetwork\n",
    "from rlzoo.common.policy_networks import StochasticPolicyNetwork\n",
    "\n",
    "''' load environment '''\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "obs_space = env.observation_space\n",
    "act_space = env.action_space\n",
    "# reproducible\n",
    "#seed = 2\n",
    "#set_seed(seed, env)\n",
    "\n",
    "''' build networks for the algorithm '''\n",
    "num_hidden_layer = 4 #number of hidden layers for the networks\n",
    "hidden_dim = 64 # dimension of hidden layers for the networks\n",
    "with tf.name_scope('AC'):\n",
    "        with tf.name_scope('Critic'):\n",
    "            \t# choose the critic network, can be replaced with customized network\n",
    "                critic = ValueNetwork(obs_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n",
    "        with tf.name_scope('Actor'):\n",
    "            \t# choose the actor network, can be replaced with customized network\n",
    "                actor = StochasticPolicyNetwork(obs_space, act_space, hidden_dim_list=num_hidden_layer * [hidden_dim], output_activation=tf.nn.tanh)\n",
    "net_list = [actor, critic] # list of the networks\n",
    "\n",
    "''' choose optimizers '''\n",
    "a_lr, c_lr = 1e-4, 1e-2  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n",
    "a_optimizer = tf.optimizers.Adam(a_lr)\n",
    "c_optimizer = tf.optimizers.Adam(c_lr)\n",
    "optimizers_list=[a_optimizer, c_optimizer]  # list of optimizers\n",
    "\n",
    "# intialize the algorithm model, with algorithm parameters passed in\n",
    "model = AC(net_list, optimizers_list)\n",
    "''' \n",
    "full list of arguments for the algorithm\n",
    "----------------------------------------\n",
    "net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\n",
    "optimizers_list: a list of optimizers for all networks and differentiable variables\n",
    "gamma: discounted factor of reward\n",
    "action_range: scale of action values\n",
    "'''\n",
    "\n",
    "# start the training process, with learning parameters passed in\n",
    "model.learn(env, train_episodes=5000,  max_steps=200,\n",
    "            save_interval=50, mode='train', render=False)\n",
    "''' \n",
    "full list of parameters for training\n",
    "---------------------------------------\n",
    "env: learning environment\n",
    "train_episodes:  total number of episodes for training\n",
    "test_episodes:  total number of episodes for testing\n",
    "max_steps:  maximum number of steps for one episode\n",
    "save_interval: time steps for saving the weights and plotting the results\n",
    "mode: 'train' or 'test'\n",
    "render:  if true, visualize the environment\n",
    "'''\n",
    "\n",
    "# test after training\n",
    "model.learn(env, test_episodes=100, max_steps=200,  mode='test', render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Input  Act_Input_Layer: (None, 1)\n",
      "[TL] Input  Obs_Input_Layer: (None, 7)\n",
      "[TL] Concat concat_1: concat_dim: -1\n",
      "[TL] Dense  mlp_hidden_layer1: 64 relu\n",
      "[TL] Dense  mlp_hidden_layer2: 64 relu\n",
      "[TL] Dense  dense_1: 1 No Activation\n",
      "[TL] Input  Act_Input_Layer: (None, 1)\n",
      "[TL] Input  Obs_Input_Layer: (None, 7)\n",
      "[TL] Concat concat_2: concat_dim: -1\n",
      "[TL] Dense  mlp_hidden_layer1: 64 relu\n",
      "[TL] Dense  mlp_hidden_layer2: 64 relu\n",
      "[TL] Dense  dense_2: 1 No Activation\n",
      "[TL] Input  Act_Input_Layer: (None, 1)\n",
      "[TL] Input  Obs_Input_Layer: (None, 7)\n",
      "[TL] Concat concat_3: concat_dim: -1\n",
      "[TL] Dense  mlp_hidden_layer1: 64 relu\n",
      "[TL] Dense  mlp_hidden_layer2: 64 relu\n",
      "[TL] Dense  dense_3: 1 No Activation\n",
      "[TL] Input  Act_Input_Layer: (None, 1)\n",
      "[TL] Input  Obs_Input_Layer: (None, 7)\n",
      "[TL] Concat concat_4: concat_dim: -1\n",
      "[TL] Dense  mlp_hidden_layer1: 64 relu\n",
      "[TL] Dense  mlp_hidden_layer2: 64 relu\n",
      "[TL] Dense  dense_4: 1 No Activation\n",
      "[TL] Input  input_layer: (None, 7)\n",
      "[TL] Dense  hidden_layer1: 64 relu\n",
      "[TL] Dense  hidden_layer2: 64 relu\n",
      "[TL] Dense  dense_5: 1 tanh\n",
      "[TL] Lambda  lambda_1: func: <function DeterministicPolicyNetwork.__init__.<locals>.<lambda> at 0x132d368c0>, len_weights: 0\n",
      "[TL] Lambda  lambda_2: func: <function DeterministicPolicyNetwork.__init__.<locals>.<lambda> at 0x132d36dd0>, len_weights: 0\n",
      "[TL] Input  input_layer: (None, 7)\n",
      "[TL] Dense  hidden_layer1: 64 relu\n",
      "[TL] Dense  hidden_layer2: 64 relu\n",
      "[TL] Dense  dense_6: 1 tanh\n",
      "[TL] Lambda  lambda_3: func: <function DeterministicPolicyNetwork.__init__.<locals>.<lambda> at 0x132d36ef0>, len_weights: 0\n",
      "[TL] Lambda  lambda_4: func: <function DeterministicPolicyNetwork.__init__.<locals>.<lambda> at 0x132d5d050>, len_weights: 0\n",
      "Q Network (1,2):  qnetwork(\n",
      "  (Obs_Input_Layer): Input(shape=(None, 7), name='Obs_Input_Layer')\n",
      "  (Act_Input_Layer): Input(shape=(None, 1), name='Act_Input_Layer')\n",
      "  (concat_1): Concat(concat_dim=-1)\n",
      "  (mlp_hidden_layer1): Dense(n_units=64, relu, in_channels='8', name='mlp_hidden_layer1')\n",
      "  (mlp_hidden_layer2): Dense(n_units=64, relu, in_channels='64', name='mlp_hidden_layer2')\n",
      "  (dense_1): Dense(n_units=1, No Activation, in_channels='64', name='dense_1')\n",
      ")\n",
      "Policy Network:  deterministicpolicynetwork(\n",
      "  (input_layer): Input(shape=(None, 7), name='input_layer')\n",
      "  (hidden_layer1): Dense(n_units=64, relu, in_channels='7', name='hidden_layer1')\n",
      "  (hidden_layer2): Dense(n_units=64, relu, in_channels='64', name='hidden_layer2')\n",
      "  (dense_5): Dense(n_units=1, tanh, in_channels='64', name='dense_5')\n",
      "  (lambda_1): Lambda(fn=<function DeterministicPolicyNetwork.__init__.<locals>.<lambda> at 0x132d368c0>,len_weights=0,name='lambda_1')\n",
      "  (lambda_2): Lambda(fn=<function DeterministicPolicyNetwork.__init__.<locals>.<lambda> at 0x132d36dd0>,len_weights=0,name='lambda_2')\n",
      ")\n",
      "Training...  | Algorithm: TD3  | Environment: WumpusWorld\n",
      "<Explorer> fell into a bottomless pit!\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_q_net1\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_q_net2\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_q_net1\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_q_net2\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_policy_net\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_policy_net\n",
      "[TL] [*] Saved\n",
      "Episode: 0/5000  | Episode Reward: -1040.0000  | Running Time: 0.2976\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 1/5000  | Episode Reward: 32.0000  | Running Time: 6.0401\n",
      "Episode: 2/5000  | Episode Reward: -200.0000  | Running Time: 6.1031\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 3/5000  | Episode Reward: -1021.0000  | Running Time: 6.8813\n",
      "Episode: 4/5000  | Episode Reward: -205.0000  | Running Time: 7.3106\n",
      "Episode: 5/5000  | Episode Reward: -200.0000  | Running Time: 7.3602\n",
      "Episode: 6/5000  | Episode Reward: -200.0000  | Running Time: 7.4234\n",
      "Episode: 7/5000  | Episode Reward: -214.0000  | Running Time: 7.7136\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 8/5000  | Episode Reward: -423.0000  | Running Time: 8.9597\n",
      "Episode: 9/5000  | Episode Reward: -212.0000  | Running Time: 9.1412\n",
      "Episode: 10/5000  | Episode Reward: -213.0000  | Running Time: 9.3627\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 11/5000  | Episode Reward: -1034.0000  | Running Time: 10.7367\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 12/5000  | Episode Reward: -939.0000  | Running Time: 18.1946\n",
      "Episode: 13/5000  | Episode Reward: -201.0000  | Running Time: 18.3028\n",
      "Episode: 14/5000  | Episode Reward: -212.0000  | Running Time: 18.4852\n",
      "Episode: 15/5000  | Episode Reward: 44.0000  | Running Time: 26.8915\n",
      "Episode: 16/5000  | Episode Reward: -202.0000  | Running Time: 27.0489\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 17/5000  | Episode Reward: -1035.0000  | Running Time: 28.5605\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 18/5000  | Episode Reward: -1026.0000  | Running Time: 29.5180\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 19/5000  | Episode Reward: -1003.0000  | Running Time: 29.6968\n",
      "Episode: 20/5000  | Episode Reward: -200.0000  | Running Time: 29.7443\n",
      "Episode: 21/5000  | Episode Reward: -214.0000  | Running Time: 30.0332\n",
      "Episode: 22/5000  | Episode Reward: -200.0000  | Running Time: 30.0968\n",
      "Episode: 23/5000  | Episode Reward: -203.0000  | Running Time: 30.3210\n",
      "Episode: 24/5000  | Episode Reward: -236.0000  | Running Time: 31.8566\n",
      "Episode: 25/5000  | Episode Reward: -212.0000  | Running Time: 32.0190\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 26/5000  | Episode Reward: -993.0000  | Running Time: 33.9523\n",
      "Episode: 27/5000  | Episode Reward: -202.0000  | Running Time: 34.1369\n",
      "Episode: 28/5000  | Episode Reward: -202.0000  | Running Time: 34.2914\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 29/5000  | Episode Reward: -1006.0000  | Running Time: 34.6388\n",
      "Episode: 30/5000  | Episode Reward: -224.0000  | Running Time: 35.5335\n",
      "Episode: 31/5000  | Episode Reward: -203.0000  | Running Time: 35.8138\n",
      "Episode: 32/5000  | Episode Reward: -217.0000  | Running Time: 36.3100\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 33/5000  | Episode Reward: -1011.0000  | Running Time: 36.9896\n",
      "Episode: 34/5000  | Episode Reward: -225.0000  | Running Time: 37.9334\n",
      "Episode: 35/5000  | Episode Reward: -212.0000  | Running Time: 38.1088\n",
      "Episode: 36/5000  | Episode Reward: -214.0000  | Running Time: 38.3946\n",
      "Episode: 37/5000  | Episode Reward: -236.0000  | Running Time: 39.9024\n",
      "Episode: 38/5000  | Episode Reward: -259.0000  | Running Time: 42.6966\n",
      "Episode: 39/5000  | Episode Reward: -203.0000  | Running Time: 42.9691\n",
      "Episode: 40/5000  | Episode Reward: -221.0000  | Running Time: 43.6469\n",
      "Episode: 41/5000  | Episode Reward: -200.0000  | Running Time: 43.7109\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 42/5000  | Episode Reward: -968.0000  | Running Time: 44.2164\n",
      "Episode: 43/5000  | Episode Reward: -201.0000  | Running Time: 44.3237\n",
      "Episode: 44/5000  | Episode Reward: -202.0000  | Running Time: 44.5081\n",
      "Episode: 45/5000  | Episode Reward: -200.0000  | Running Time: 44.5701\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 46/5000  | Episode Reward: -825.0000  | Running Time: 45.5763\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 47/5000  | Episode Reward: -1018.0000  | Running Time: 46.0330\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 48/5000  | Episode Reward: -1091.0000  | Running Time: 50.5844\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 49/5000  | Episode Reward: -1024.0000  | Running Time: 51.3750\n",
      "Episode: 50/5000  | Episode Reward: -200.0000  | Running Time: 51.4422\n",
      "Episode: 51/5000  | Episode Reward: 910.0000  | Running Time: 59.8586\n",
      "Episode: 52/5000  | Episode Reward: -201.0000  | Running Time: 59.9679\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 53/5000  | Episode Reward: -1005.0000  | Running Time: 60.2705\n",
      "Episode: 54/5000  | Episode Reward: -249.0000  | Running Time: 62.5134\n",
      "Episode: 55/5000  | Episode Reward: -221.0000  | Running Time: 63.1977\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 56/5000  | Episode Reward: -60.0000  | Running Time: 70.0670\n",
      "Episode: 57/5000  | Episode Reward: -235.0000  | Running Time: 71.6023\n",
      "Episode: 58/5000  | Episode Reward: -7.0000  | Running Time: 80.0897\n",
      "Episode: 59/5000  | Episode Reward: -201.0000  | Running Time: 80.1980\n",
      "Episode: 60/5000  | Episode Reward: -200.0000  | Running Time: 80.2609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 61/5000  | Episode Reward: -223.0000  | Running Time: 81.0639\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 62/5000  | Episode Reward: 58.0000  | Running Time: 83.9935\n",
      "Episode: 63/5000  | Episode Reward: -201.0000  | Running Time: 84.1023\n",
      "Episode: 64/5000  | Episode Reward: -216.0000  | Running Time: 84.4978\n",
      "Episode: 65/5000  | Episode Reward: -219.0000  | Running Time: 85.0573\n",
      "Episode: 66/5000  | Episode Reward: -200.0000  | Running Time: 85.1203\n",
      "Episode: 67/5000  | Episode Reward: -200.0000  | Running Time: 85.1662\n",
      "Episode: 68/5000  | Episode Reward: -233.0000  | Running Time: 86.5034\n",
      "Episode: 69/5000  | Episode Reward: -200.0000  | Running Time: 86.5560\n",
      "Episode: 70/5000  | Episode Reward: -205.0000  | Running Time: 86.8985\n",
      "Episode: 71/5000  | Episode Reward: -201.0000  | Running Time: 87.0111\n",
      "Episode: 72/5000  | Episode Reward: 399.0000  | Running Time: 95.4552\n",
      "Episode: 73/5000  | Episode Reward: 55.0000  | Running Time: 103.4661\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 74/5000  | Episode Reward: -1100.0000  | Running Time: 108.5198\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 75/5000  | Episode Reward: -1033.0000  | Running Time: 109.8255\n",
      "Episode: 76/5000  | Episode Reward: -200.0000  | Running Time: 109.8902\n",
      "Episode: 77/5000  | Episode Reward: -222.0000  | Running Time: 110.6402\n",
      "Episode: 78/5000  | Episode Reward: -201.0000  | Running Time: 110.7521\n",
      "Episode: 79/5000  | Episode Reward: -214.0000  | Running Time: 111.0411\n",
      "Episode: 80/5000  | Episode Reward: -201.0000  | Running Time: 111.1503\n",
      "Episode: 81/5000  | Episode Reward: -205.0000  | Running Time: 111.5047\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 82/5000  | Episode Reward: -1019.0000  | Running Time: 112.0151\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 83/5000  | Episode Reward: -1070.0000  | Running Time: 115.3219\n",
      "Episode: 84/5000  | Episode Reward: -200.0000  | Running Time: 115.3881\n",
      "Episode: 85/5000  | Episode Reward: -243.0000  | Running Time: 117.2602\n",
      "Episode: 86/5000  | Episode Reward: -160.0000  | Running Time: 125.5876\n",
      "Episode: 87/5000  | Episode Reward: -201.0000  | Running Time: 125.6950\n",
      "Episode: 88/5000  | Episode Reward: -212.0000  | Running Time: 125.8770\n",
      "Episode: 89/5000  | Episode Reward: -202.0000  | Running Time: 126.0497\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 90/5000  | Episode Reward: -1035.0000  | Running Time: 127.4431\n",
      "Episode: 91/5000  | Episode Reward: -214.0000  | Running Time: 127.7244\n",
      "Episode: 92/5000  | Episode Reward: -200.0000  | Running Time: 127.7725\n",
      "Episode: 93/5000  | Episode Reward: -227.0000  | Running Time: 128.7916\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 94/5000  | Episode Reward: -1016.0000  | Running Time: 129.1686\n",
      "Episode: 95/5000  | Episode Reward: -216.0000  | Running Time: 129.5621\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 96/5000  | Episode Reward: -1002.0000  | Running Time: 129.6705\n",
      "Episode: 97/5000  | Episode Reward: -201.0000  | Running Time: 129.7850\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 98/5000  | Episode Reward: -1007.0000  | Running Time: 130.1986\n",
      "Episode: 99/5000  | Episode Reward: -200.0000  | Running Time: 130.2457\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_q_net1\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_q_net2\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_q_net1\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_q_net2\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_policy_net\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_policy_net\n",
      "[TL] [*] Saved\n",
      "Episode: 100/5000  | Episode Reward: -213.0000  | Running Time: 130.6829\n",
      "Episode: 101/5000  | Episode Reward: -220.0000  | Running Time: 131.3173\n",
      "Episode: 102/5000  | Episode Reward: -201.0000  | Running Time: 131.4299\n",
      "Episode: 103/5000  | Episode Reward: -201.0000  | Running Time: 131.5633\n",
      "Episode: 104/5000  | Episode Reward: -202.0000  | Running Time: 131.7316\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 105/5000  | Episode Reward: -1002.0000  | Running Time: 131.9139\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 106/5000  | Episode Reward: -811.0000  | Running Time: 135.0826\n",
      "Episode: 107/5000  | Episode Reward: -217.0000  | Running Time: 135.5428\n",
      "Episode: 108/5000  | Episode Reward: -224.0000  | Running Time: 136.4762\n",
      "Episode: 109/5000  | Episode Reward: -225.0000  | Running Time: 137.8981\n",
      "Episode: 110/5000  | Episode Reward: -213.0000  | Running Time: 138.1588\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 111/5000  | Episode Reward: -1032.0000  | Running Time: 140.4907\n",
      "Episode: 112/5000  | Episode Reward: -202.0000  | Running Time: 140.7917\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 113/5000  | Episode Reward: -1033.0000  | Running Time: 142.7428\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 114/5000  | Episode Reward: -1151.0000  | Running Time: 151.2423\n",
      "Episode: 115/5000  | Episode Reward: -223.0000  | Running Time: 152.0436\n",
      "Episode: 116/5000  | Episode Reward: -200.0000  | Running Time: 152.0905\n",
      "Episode: 117/5000  | Episode Reward: -212.0000  | Running Time: 152.2671\n",
      "Episode: 118/5000  | Episode Reward: -202.0000  | Running Time: 152.4380\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 119/5000  | Episode Reward: -1015.0000  | Running Time: 152.7241\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 120/5000  | Episode Reward: -1017.0000  | Running Time: 153.1616\n",
      "Episode: 121/5000  | Episode Reward: -214.0000  | Running Time: 153.4582\n",
      "Episode: 122/5000  | Episode Reward: -211.0000  | Running Time: 154.1438\n",
      "Episode: 123/5000  | Episode Reward: -211.0000  | Running Time: 154.2711\n",
      "Episode: 124/5000  | Episode Reward: -200.0000  | Running Time: 154.3167\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 125/5000  | Episode Reward: -1020.0000  | Running Time: 154.8717\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 126/5000  | Episode Reward: -817.0000  | Running Time: 161.2239\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 127/5000  | Episode Reward: -1004.0000  | Running Time: 161.4708\n",
      "Episode: 128/5000  | Episode Reward: -203.0000  | Running Time: 161.6996\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 129/5000  | Episode Reward: -1032.0000  | Running Time: 162.9341\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 130/5000  | Episode Reward: -1002.0000  | Running Time: 163.0669\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 131/5000  | Episode Reward: -1002.0000  | Running Time: 163.1833\n",
      "Episode: 132/5000  | Episode Reward: -214.0000  | Running Time: 163.4693\n",
      "Episode: 133/5000  | Episode Reward: -217.0000  | Running Time: 163.9248\n",
      "Episode: 134/5000  | Episode Reward: -200.0000  | Running Time: 163.9711\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 135/5000  | Episode Reward: -1024.0000  | Running Time: 164.7821\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 136/5000  | Episode Reward: -1040.0000  | Running Time: 166.4790\n",
      "Episode: 137/5000  | Episode Reward: -200.0000  | Running Time: 166.5254\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 138/5000  | Episode Reward: -1039.0000  | Running Time: 168.1669\n",
      "Episode: 139/5000  | Episode Reward: -234.0000  | Running Time: 169.5791\n",
      "Episode: 140/5000  | Episode Reward: -201.0000  | Running Time: 169.6878\n",
      "Episode: 141/5000  | Episode Reward: -228.0000  | Running Time: 170.7597\n",
      "Episode: 142/5000  | Episode Reward: -202.0000  | Running Time: 170.9306\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 143/5000  | Episode Reward: -1036.0000  | Running Time: 172.7632\n",
      "Episode: 144/5000  | Episode Reward: -216.0000  | Running Time: 173.3687\n",
      "Episode: 145/5000  | Episode Reward: -247.0000  | Running Time: 175.5552\n",
      "Episode: 146/5000  | Episode Reward: -200.0000  | Running Time: 175.6009\n",
      "Episode: 147/5000  | Episode Reward: -200.0000  | Running Time: 175.6648\n",
      "Episode: 148/5000  | Episode Reward: -220.0000  | Running Time: 176.2843\n",
      "Episode: 149/5000  | Episode Reward: -213.0000  | Running Time: 176.5259\n",
      "Episode: 150/5000  | Episode Reward: -200.0000  | Running Time: 176.5719\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 151/5000  | Episode Reward: -1032.0000  | Running Time: 177.8226\n",
      "Episode: 152/5000  | Episode Reward: -200.0000  | Running Time: 177.8684\n",
      "Episode: 153/5000  | Episode Reward: -160.0000  | Running Time: 186.3786\n",
      "Episode: 154/5000  | Episode Reward: -202.0000  | Running Time: 186.5665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 155/5000  | Episode Reward: -221.0000  | Running Time: 187.2478\n",
      "Episode: 156/5000  | Episode Reward: -201.0000  | Running Time: 187.3606\n",
      "Episode: 157/5000  | Episode Reward: -215.0000  | Running Time: 187.7152\n",
      "Episode: 158/5000  | Episode Reward: -224.0000  | Running Time: 188.5799\n",
      "Episode: 159/5000  | Episode Reward: -200.0000  | Running Time: 188.6444\n",
      "Episode: 160/5000  | Episode Reward: -200.0000  | Running Time: 188.6910\n",
      "Episode: 161/5000  | Episode Reward: -279.0000  | Running Time: 192.6922\n",
      "Episode: 162/5000  | Episode Reward: -217.0000  | Running Time: 193.1562\n",
      "Episode: 163/5000  | Episode Reward: -200.0000  | Running Time: 193.2334\n",
      "Episode: 164/5000  | Episode Reward: -201.0000  | Running Time: 193.3444\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 165/5000  | Episode Reward: -1002.0000  | Running Time: 193.4592\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 166/5000  | Episode Reward: -1014.0000  | Running Time: 193.6837\n",
      "Episode: 167/5000  | Episode Reward: -215.0000  | Running Time: 194.0507\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 168/5000  | Episode Reward: -1041.0000  | Running Time: 195.7982\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 169/5000  | Episode Reward: -1018.0000  | Running Time: 196.2446\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 170/5000  | Episode Reward: -1090.0000  | Running Time: 200.7208\n",
      "Episode: 171/5000  | Episode Reward: -243.0000  | Running Time: 202.7565\n",
      "Episode: 172/5000  | Episode Reward: -217.0000  | Running Time: 203.3492\n",
      "Episode: 173/5000  | Episode Reward: -203.0000  | Running Time: 203.5707\n",
      "Episode: 174/5000  | Episode Reward: 378.0000  | Running Time: 614.2199\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 175/5000  | Episode Reward: -1122.0000  | Running Time: 624.4467\n",
      "Episode: 176/5000  | Episode Reward: -225.0000  | Running Time: 625.4895\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 177/5000  | Episode Reward: -1002.0000  | Running Time: 625.6119\n",
      "Episode: 178/5000  | Episode Reward: -200.0000  | Running Time: 625.6881\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 179/5000  | Episode Reward: -1005.0000  | Running Time: 626.0740\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 180/5000  | Episode Reward: -1102.0000  | Running Time: 631.4367\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 181/5000  | Episode Reward: -1003.0000  | Running Time: 631.6174\n",
      "Episode: 182/5000  | Episode Reward: -219.0000  | Running Time: 632.2036\n",
      "Episode: 183/5000  | Episode Reward: -216.0000  | Running Time: 632.5964\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 184/5000  | Episode Reward: -1002.0000  | Running Time: 632.7101\n",
      "Episode: 185/5000  | Episode Reward: -218.0000  | Running Time: 633.2369\n",
      "Episode: 186/5000  | Episode Reward: -200.0000  | Running Time: 633.2841\n",
      "Episode: 187/5000  | Episode Reward: -203.0000  | Running Time: 633.5206\n",
      "Episode: 188/5000  | Episode Reward: -200.0000  | Running Time: 633.5703\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 189/5000  | Episode Reward: -1016.0000  | Running Time: 633.9258\n",
      "Episode: 190/5000  | Episode Reward: -263.0000  | Running Time: 637.0112\n",
      "Episode: 191/5000  | Episode Reward: -207.0000  | Running Time: 637.5897\n",
      "Episode: 192/5000  | Episode Reward: -213.0000  | Running Time: 637.9247\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 193/5000  | Episode Reward: -1027.0000  | Running Time: 639.1421\n",
      "Episode: 194/5000  | Episode Reward: -204.0000  | Running Time: 639.4775\n",
      "Episode: 195/5000  | Episode Reward: -200.0000  | Running Time: 639.5300\n",
      "Episode: 196/5000  | Episode Reward: -211.0000  | Running Time: 639.6756\n",
      "Episode: 197/5000  | Episode Reward: -201.0000  | Running Time: 639.8092\n",
      "Episode: 198/5000  | Episode Reward: -200.0000  | Running Time: 639.8582\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 199/5000  | Episode Reward: -1004.0000  | Running Time: 640.1277\n",
      "A Wumpus ate <Explorer>!\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_q_net1\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_q_net2\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_q_net1\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_q_net2\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_policy_net\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_policy_net\n",
      "[TL] [*] Saved\n",
      "Episode: 200/5000  | Episode Reward: -1023.0000  | Running Time: 641.2611\n",
      "Episode: 201/5000  | Episode Reward: -200.0000  | Running Time: 641.3311\n",
      "Episode: 202/5000  | Episode Reward: -200.0000  | Running Time: 641.4142\n",
      "Episode: 203/5000  | Episode Reward: -201.0000  | Running Time: 641.5647\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 204/5000  | Episode Reward: -1043.0000  | Running Time: 643.5832\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 205/5000  | Episode Reward: -1025.0000  | Running Time: 644.4622\n",
      "Episode: 206/5000  | Episode Reward: -201.0000  | Running Time: 644.5761\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 207/5000  | Episode Reward: -1044.0000  | Running Time: 646.5235\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 208/5000  | Episode Reward: -1007.0000  | Running Time: 646.9575\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 209/5000  | Episode Reward: -1018.0000  | Running Time: 647.4934\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 210/5000  | Episode Reward: -1025.0000  | Running Time: 648.4403\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 211/5000  | Episode Reward: -1017.0000  | Running Time: 648.8773\n",
      "Episode: 212/5000  | Episode Reward: -200.0000  | Running Time: 648.9362\n",
      "Episode: 213/5000  | Episode Reward: -217.0000  | Running Time: 649.3946\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 214/5000  | Episode Reward: -783.0000  | Running Time: 651.0041\n",
      "Episode: 215/5000  | Episode Reward: -200.0000  | Running Time: 651.0498\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 216/5000  | Episode Reward: -823.0000  | Running Time: 655.2127\n",
      "Episode: 217/5000  | Episode Reward: -200.0000  | Running Time: 655.2924\n",
      "Episode: 218/5000  | Episode Reward: -212.0000  | Running Time: 655.4922\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 219/5000  | Episode Reward: -1047.0000  | Running Time: 657.9915\n",
      "Episode: 220/5000  | Episode Reward: -203.0000  | Running Time: 658.3208\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 221/5000  | Episode Reward: -1002.0000  | Running Time: 658.4666\n",
      "Episode: 222/5000  | Episode Reward: -223.0000  | Running Time: 659.3853\n",
      "Episode: 223/5000  | Episode Reward: -214.0000  | Running Time: 659.7164\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 224/5000  | Episode Reward: -1103.0000  | Running Time: 665.5699\n",
      "Episode: 225/5000  | Episode Reward: -235.0000  | Running Time: 667.4265\n",
      "Episode: 226/5000  | Episode Reward: -218.0000  | Running Time: 667.9461\n",
      "Episode: 227/5000  | Episode Reward: -218.0000  | Running Time: 668.5953\n",
      "Episode: 228/5000  | Episode Reward: -227.0000  | Running Time: 669.9814\n",
      "Episode: 229/5000  | Episode Reward: -214.0000  | Running Time: 670.3014\n",
      "Episode: 230/5000  | Episode Reward: -201.0000  | Running Time: 670.4329\n",
      "Episode: 231/5000  | Episode Reward: -244.0000  | Running Time: 672.7781\n",
      "Episode: 232/5000  | Episode Reward: -200.0000  | Running Time: 672.8439\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 233/5000  | Episode Reward: -1006.0000  | Running Time: 673.2478\n",
      "Episode: 234/5000  | Episode Reward: -200.0000  | Running Time: 673.2979\n",
      "Episode: 235/5000  | Episode Reward: -246.0000  | Running Time: 675.9157\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 236/5000  | Episode Reward: -1002.0000  | Running Time: 676.0448\n",
      "Episode: 237/5000  | Episode Reward: -225.0000  | Running Time: 677.0060\n",
      "Episode: 238/5000  | Episode Reward: -203.0000  | Running Time: 677.3037\n",
      "Episode: 239/5000  | Episode Reward: -212.0000  | Running Time: 677.4888\n",
      "Episode: 240/5000  | Episode Reward: -211.0000  | Running Time: 677.6810\n",
      "Episode: 241/5000  | Episode Reward: -221.0000  | Running Time: 678.5530\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 242/5000  | Episode Reward: -1002.0000  | Running Time: 678.7221\n",
      "Episode: 243/5000  | Episode Reward: -201.0000  | Running Time: 678.8669\n",
      "Episode: 244/5000  | Episode Reward: -200.0000  | Running Time: 678.9367\n",
      "Episode: 245/5000  | Episode Reward: -200.0000  | Running Time: 678.9908\n",
      "Episode: 246/5000  | Episode Reward: -206.0000  | Running Time: 679.4723\n",
      "Episode: 247/5000  | Episode Reward: -200.0000  | Running Time: 679.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 248/5000  | Episode Reward: -214.0000  | Running Time: 679.8502\n",
      "Episode: 249/5000  | Episode Reward: -235.0000  | Running Time: 681.5579\n",
      "Episode: 250/5000  | Episode Reward: -201.0000  | Running Time: 681.6794\n",
      "Episode: 251/5000  | Episode Reward: -245.0000  | Running Time: 683.9910\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 252/5000  | Episode Reward: 11.0000  | Running Time: 693.4546\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 253/5000  | Episode Reward: -1028.0000  | Running Time: 694.6862\n",
      "Episode: 254/5000  | Episode Reward: -201.0000  | Running Time: 694.8072\n",
      "Episode: 255/5000  | Episode Reward: -216.0000  | Running Time: 695.2732\n",
      "Episode: 256/5000  | Episode Reward: -213.0000  | Running Time: 695.5147\n",
      "Episode: 257/5000  | Episode Reward: -214.0000  | Running Time: 695.8413\n",
      "Episode: 258/5000  | Episode Reward: -200.0000  | Running Time: 695.9101\n",
      "Episode: 259/5000  | Episode Reward: -76.0000  | Running Time: 700.3466\n",
      "Episode: 260/5000  | Episode Reward: -202.0000  | Running Time: 700.5683\n",
      "Episode: 261/5000  | Episode Reward: -201.0000  | Running Time: 700.7169\n",
      "Episode: 262/5000  | Episode Reward: -221.0000  | Running Time: 701.5155\n",
      "Episode: 263/5000  | Episode Reward: -200.0000  | Running Time: 701.5687\n",
      "Episode: 264/5000  | Episode Reward: -201.0000  | Running Time: 701.7259\n",
      "Episode: 265/5000  | Episode Reward: -202.0000  | Running Time: 701.9187\n",
      "Episode: 266/5000  | Episode Reward: -205.0000  | Running Time: 702.3313\n",
      "Episode: 267/5000  | Episode Reward: 1316.0000  | Running Time: 712.8451\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 268/5000  | Episode Reward: -1089.0000  | Running Time: 718.5434\n",
      "Episode: 269/5000  | Episode Reward: -150.0000  | Running Time: 727.8158\n",
      "Episode: 270/5000  | Episode Reward: -150.0000  | Running Time: 737.9634\n",
      "Episode: 271/5000  | Episode Reward: -150.0000  | Running Time: 747.6616\n",
      "Episode: 272/5000  | Episode Reward: -150.0000  | Running Time: 756.9521\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 273/5000  | Episode Reward: -945.0000  | Running Time: 763.1438\n",
      "Episode: 274/5000  | Episode Reward: -150.0000  | Running Time: 772.8251\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 275/5000  | Episode Reward: -659.0000  | Running Time: 777.1154\n",
      "Episode: 276/5000  | Episode Reward: -150.0000  | Running Time: 786.4460\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 277/5000  | Episode Reward: -1114.0000  | Running Time: 793.5387\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 278/5000  | Episode Reward: -1030.0000  | Running Time: 795.4468\n",
      "Episode: 279/5000  | Episode Reward: -150.0000  | Running Time: 804.7665\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 280/5000  | Episode Reward: -1112.0000  | Running Time: 811.8522\n",
      "Episode: 281/5000  | Episode Reward: -150.0000  | Running Time: 821.6910\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 282/5000  | Episode Reward: -1140.0000  | Running Time: 831.0490\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 283/5000  | Episode Reward: -1065.0000  | Running Time: 835.3872\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 284/5000  | Episode Reward: -1033.0000  | Running Time: 837.5722\n",
      "Episode: 285/5000  | Episode Reward: -150.0000  | Running Time: 846.9481\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 286/5000  | Episode Reward: -1067.0000  | Running Time: 851.1816\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 287/5000  | Episode Reward: -1039.0000  | Running Time: 853.6022\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 288/5000  | Episode Reward: -1055.0000  | Running Time: 857.1811\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 289/5000  | Episode Reward: -1089.0000  | Running Time: 863.2441\n",
      "Episode: 290/5000  | Episode Reward: 1533.0000  | Running Time: 873.2328\n",
      "Episode: 291/5000  | Episode Reward: -150.0000  | Running Time: 882.6970\n",
      "Episode: 292/5000  | Episode Reward: -150.0000  | Running Time: 892.1487\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 293/5000  | Episode Reward: -1150.0000  | Running Time: 902.5123\n",
      "Episode: 294/5000  | Episode Reward: -150.0000  | Running Time: 914.2448\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 295/5000  | Episode Reward: -1132.0000  | Running Time: 922.8435\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 296/5000  | Episode Reward: -1092.0000  | Running Time: 929.4614\n",
      "Episode: 297/5000  | Episode Reward: -150.0000  | Running Time: 939.4475\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 298/5000  | Episode Reward: -1044.0000  | Running Time: 942.4790\n",
      "<Explorer> fell into a bottomless pit!\n",
      "Episode: 299/5000  | Episode Reward: -1018.0000  | Running Time: 943.6646\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_q_net1\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_q_net2\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_q_net1\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_q_net2\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_policy_net\n",
      "[TL] [*] Saved\n",
      "[TL] [*] Saving TL weights into ./model/TD3-WumpusWorld/model_target_policy_net\n",
      "[TL] [*] Saved\n",
      "Episode: 300/5000  | Episode Reward: -150.0000  | Running Time: 954.3329\n",
      "Episode: 301/5000  | Episode Reward: -150.0000  | Running Time: 965.1355\n",
      "A Wumpus ate <Explorer>!\n",
      "Episode: 302/5000  | Episode Reward: -1004.0000  | Running Time: 965.4702\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-63a8e50525ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m model.learn(env, train_episodes=5000, max_steps=150, batch_size=64, explore_steps=5000, update_itr=3,\n\u001b[1;32m     61\u001b[0m             \u001b[0mreward_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplore_noise_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_noise_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             render=False)\n\u001b[0m\u001b[1;32m     63\u001b[0m ''' \n\u001b[1;32m     64\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wumpus-rl/lib/python3.7/site-packages/rlzoo/algorithms/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, env, train_episodes, test_episodes, max_steps, batch_size, explore_steps, update_itr, reward_scale, save_interval, explore_noise_scale, eval_noise_scale, mode, render)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_itr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_noise_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_noise_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wumpus-rl/lib/python3.7/site-packages/rlzoo/algorithms/td3/td3.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch_size, eval_noise_scale, reward_scale, gamma, soft_tau)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mq_value_loss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_q_value2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget_q_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mq2_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq2_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_optimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Training Policy Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wumpus-rl/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    439\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m           kwargs={\"name\": name})\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wumpus-rl/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wumpus-rl/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   1923\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wumpus-rl/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    482\u001b[0m             scope_name), distribution.extended.colocate_vars_with(var):\n\u001b[1;32m    483\u001b[0m           update_ops.extend(\n\u001b[0;32m--> 484\u001b[0;31m               distribution.extended.update(\n\u001b[0m\u001b[1;32m    485\u001b[0m                   var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wumpus-rl/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mextended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;34m\"\"\"`tf.distribute.StrategyExtended` with additional methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf_contextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rlzoo.common.utils import make_env\n",
    "from rlzoo.algorithms.td3.td3 import TD3\n",
    "from rlzoo.common.value_networks import *\n",
    "from rlzoo.common.policy_networks import *\n",
    "\n",
    "''' load environment '''\n",
    "# env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized/wrapped environment to run\n",
    "action_shape = env.action_space.shape\n",
    "state_shape = env.observation_space.shape\n",
    "# reproducible\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "#env.seed(seed)\n",
    "\n",
    "''' build networks for the algorithm '''\n",
    "num_hidden_layer = 2  # number of hidden layers for the networks\n",
    "hidden_dim = 64  # dimension of hidden layers for the networks\n",
    "with tf.name_scope('TD3'):\n",
    "    with tf.name_scope('Q_Net1'):\n",
    "        q_net1 = QNetwork(env.observation_space, env.action_space,\n",
    "                          hidden_dim_list=num_hidden_layer * [hidden_dim])\n",
    "    with tf.name_scope('Q_Net2'):\n",
    "        q_net2 = QNetwork(env.observation_space, env.action_space,\n",
    "                          hidden_dim_list=num_hidden_layer * [hidden_dim])\n",
    "    with tf.name_scope('Target_Q_Net1'):\n",
    "        target_q_net1 = QNetwork(env.observation_space, env.action_space,\n",
    "                                 hidden_dim_list=num_hidden_layer * [hidden_dim])\n",
    "    with tf.name_scope('Target_Q_Net2'):\n",
    "        target_q_net2 = QNetwork(env.observation_space, env.action_space,\n",
    "                                 hidden_dim_list=num_hidden_layer * [hidden_dim])\n",
    "    with tf.name_scope('Policy'):\n",
    "        policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n",
    "                                                hidden_dim_list=num_hidden_layer * [hidden_dim])\n",
    "    with tf.name_scope('Target_Policy'):\n",
    "        target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n",
    "                                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n",
    "net_list = [q_net1, q_net2, target_q_net1, target_q_net2, policy_net, target_policy_net]\n",
    "\n",
    "''' choose optimizers '''\n",
    "q_lr, policy_lr = 3e-4, 3e-4  # q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network\n",
    "q_optimizer1 = tf.optimizers.Adam(q_lr)\n",
    "q_optimizer2 = tf.optimizers.Adam(q_lr)\n",
    "policy_optimizer = tf.optimizers.Adam(policy_lr)\n",
    "optimizers_list = [q_optimizer1, q_optimizer2, policy_optimizer]\n",
    "\n",
    "model = TD3(net_list, optimizers_list)\n",
    "''' \n",
    "full list of arguments for the algorithm\n",
    "----------------------------------------\n",
    "net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\n",
    "optimizers_list: a list of optimizers for all networks and differentiable variables\n",
    "state_dim: dimension of state for the environment\n",
    "action_dim: dimension of action for the environment\n",
    "replay_buffer_capacity: the size of buffer for storing explored samples\n",
    "policy_target_update_interval: delayed interval for updating the target policy\n",
    "action_range: value of each action in [-action_range, action_range]\n",
    "'''\n",
    "\n",
    "model.learn(env, train_episodes=5000, max_steps=150, batch_size=64, explore_steps=5000, update_itr=3,\n",
    "            reward_scale=1., save_interval=100, explore_noise_scale=1.0, eval_noise_scale=0.5, mode='train',\n",
    "            render=False)\n",
    "''' \n",
    "full list of parameters for training\n",
    "---------------------------------------\n",
    "env: learning environment\n",
    "train_episodes:  total number of episodes for training\n",
    "test_episodes:  total number of episodes for testing\n",
    "max_steps:  maximum number of steps for one episode\n",
    "batch_size:  udpate batchsize\n",
    "explore_steps:  for random action sampling in the beginning of training\n",
    "update_itr: repeated updates for single step\n",
    "reward_scale: value range of reward\n",
    "save_interval: timesteps for saving the weights and plotting the results\n",
    "explore_noise_scale: range of action noise for exploration\n",
    "eval_noise_scale: range of action noise for evaluation of action value\n",
    "mode: 'train' or 'test'\n",
    "render: if true, visualize the environment\n",
    "'''\n",
    "# test\n",
    "model.learn(env, test_episodes=10, max_steps=150, mode='test', render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
